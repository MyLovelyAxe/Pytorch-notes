{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b527c78c",
   "metadata": {},
   "source": [
    "## Loss Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0522208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdabdfc",
   "metadata": {},
   "source": [
    "#### MSE: Mean Square Error\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "loss &= \\sum [y - (wx + b)]^{2} \\\\\n",
    "&= \\sum [y - f_{\\theta}(x)]^{2} \\\\\n",
    "\\frac{\\partial{loss}}{\\partial{\\theta}} &= 2 \\cdot \\sum \\left( [y - f_{\\theta}(x)] \\cdot \\frac{\\partial{f_{\\theta}(x)}}{\\partial{\\theta}} \\right) \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$loss$: error between target **y** and prediction/output; \n",
    "$y$: target;\n",
    "$w$: weights;\n",
    "$x$: input;\n",
    "$b$: bias;\n",
    "\n",
    "$f_{\\theta}(x)$: function **f** with parameters **$\\theta$** which is same with $wx + b$;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b458e2",
   "metadata": {},
   "source": [
    "#### Method 1 to get gradient: autograd.grad( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cadd7ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input is tensor([1.]), weight is tensor([2.], requires_grad=True), target is: tensor([1.]), output is tensor([4.], grad_fn=<AddBackward0>)\n",
      "loss is 9.0\n",
      "gradient is: (tensor([6.]), tensor([6.])) with type of: <class 'tuple'>\n",
      "gradient of weight is: tensor([6.]) with type of: <class 'torch.Tensor'>\n",
      "gradient of bias is: tensor([6.]) with type of: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# input x: scalar with value 1.\n",
    "x = torch.ones(1)\n",
    "# weight w: scalar with value 2.\n",
    "# gradient of weight w is needed, so set feature 'requires_grad' as True\n",
    "w = torch.full([1],2., requires_grad=True)\n",
    "# bias b: scalar with value 2.\n",
    "# gradient of bias b is needed, so set feature 'requires_grad' as True\n",
    "b = torch.full([1],2., requires_grad=True)\n",
    "# target t: scalar with value 1.\n",
    "t = torch.ones(1)\n",
    "# prediction/ output y: w*x\n",
    "y = w*x + b\n",
    "print('input is {}, weight is {}, target is: {}, output is {}'.format(x,w,t,y))\n",
    "# MSE: torch.nn.functional.mse_loss(target,output)\n",
    "loss = F.mse_loss(t,y)\n",
    "print('loss is {}'.format(loss))\n",
    "# get gradient of weight w and bias b\n",
    "# attention: all parameters which need gradients should be input as a list, e.g. [w,b]\n",
    "grad = torch.autograd.grad(loss,[w,b])\n",
    "print('gradient is: {} with type of: {}'.format(grad, type(grad)))\n",
    "print('gradient of weight is: {} with type of: {}'.format(grad[0], type(grad[0])))\n",
    "print('gradient of bias is: {} with type of: {}'.format(grad[1], type(grad[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c688f1ae",
   "metadata": {},
   "source": [
    "#### Method 2 to get gradient: loss.backward( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fb55043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input is tensor([1.]), weight is tensor([2.], requires_grad=True), target is: tensor([1.]), output is tensor([4.], grad_fn=<AddBackward0>)\n",
      "loss is 9.0\n",
      "gradient of weight is: tensor([6.]) with type of: <class 'torch.Tensor'>\n",
      "gradient of bias is: tensor([6.]) with type of: <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1)\n",
    "w = torch.full([1],2., requires_grad=True)\n",
    "b = torch.full([1],2., requires_grad=True)\n",
    "t = torch.ones(1)\n",
    "y = w*x + b\n",
    "print('input is {}, weight is {}, target is: {}, output is {}'.format(x,w,t,y))\n",
    "loss = F.mse_loss(t,y)\n",
    "print('loss is {}'.format(loss))\n",
    "# apply backpropagation to get gradients of parameters\n",
    "loss.backward()\n",
    "w_grad = w.grad\n",
    "print('gradient of weight is: {} with type of: {}'.format(w_grad, type(w_grad)))\n",
    "b_grad = b.grad\n",
    "print('gradient of bias is: {} with type of: {}'.format(b_grad, type(b_grad)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeb2213",
   "metadata": {},
   "source": [
    "#### Softmax: soft version of max\n",
    "\n",
    "<font size=2>\n",
    "\n",
    "squeeze all elements into scale of (0,1), make them as probabilities whose summation is 1. Take the element with largest propability to be **max**.\n",
    "\n",
    "Assume we have a output with **N-dimension**, which means:\n",
    "\n",
    "$$ output = [a_{1},a_{2},...,a_{N}] $$\n",
    "\n",
    "and softmax value of $a_{i}$ from output is denoted as $p_{i}$:\n",
    "    \n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "p_{i} &= \\frac{e^{a_{i}}}{\\sum^{N}_{j} e^{a_{j}}} \\\\\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "    \n",
    "derivatives of softmax is:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial{p_{i}}}{\\partial{a_{j}}} = \\begin{cases}\n",
    "p_{i}(1 - p_{j}) & i = j \\\\ - p_{j}p_{i} & i \\neq j\n",
    "\\end{cases}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ed46d975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob:  tensor([0.3333, 0.3333, 0.3333], grad_fn=<SoftmaxBackward0>)\n",
      "grad can be implicitly created only for scalar outputs\n"
     ]
    }
   ],
   "source": [
    "# output: contain 3 values\n",
    "# elements in output need gradient, set 'requires_grad' as True\n",
    "output = torch.tensor([1.,1.,1.], requires_grad=True)\n",
    "# prob: output is converted into probability by softmax\n",
    "# along with dim0\n",
    "prob = F.softmax(output, dim=0)\n",
    "print('prob: ', prob)\n",
    "# attention:\n",
    "# backpropagation is usually carried out with one 'error/loss' value to all parameters\n",
    "# and 'softmax' is not an 'error/loss' with only one value, but several probabilities\n",
    "# we can separately obtain gradients of some specific one probability to parameters\n",
    "# in this case, it just shows the feature of 'taking gradients by error'\n",
    "# so in this case '.backward()' doesn't work well, then see next case\n",
    "try:\n",
    "    prob.backward(retain_graph=True)\n",
    "    o_grad = output.grad\n",
    "    print('gradient of output is {}'.format(o_grad))\n",
    "except RuntimeError as e:\n",
    "    # RuntimeError: grad can be implicitly created only for scalar outputs\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "88019707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob:  tensor([0.3333, 0.3333, 0.3333], grad_fn=<SoftmaxBackward0>)\n",
      "gradient of prob[0] r.w.t output is (tensor([ 0.2222, -0.1111, -0.1111]),)\n",
      "gradient of prob[1] r.w.t output is (tensor([-0.1111,  0.2222, -0.1111]),)\n",
      "gradient of prob[2] r.w.t output is (tensor([-0.1111, -0.1111,  0.2222]),)\n"
     ]
    }
   ],
   "source": [
    "# output: contain 3 values\n",
    "# elements in output need gradient, set 'requires_grad' as True\n",
    "output = torch.tensor([1.,1.,1.], requires_grad=True)\n",
    "# prob: output is converted into probability by softmax\n",
    "# along with dim0\n",
    "prob = F.softmax(output, dim=0)\n",
    "print('prob: ', prob)\n",
    "# compared to previous case\n",
    "# take gradients separately of each softmax value w.r.t parameters (here 'output')\n",
    "for i in range(output.shape[0]):\n",
    "    o_grad = torch.autograd.grad(prob[i], output, retain_graph=True)\n",
    "    print('gradient of prob[{}] r.w.t output is {}'.format(i,o_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89b8e51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
