{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5059accd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76ba4a9",
   "metadata": {},
   "source": [
    "#### torch.nn.Conv2d( )\n",
    "create 2d-CNN layer by class: torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63c9b76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------\n",
      "torch.nn.Conv2d()\n",
      "the size of input is: torch.Size([1, 1, 28, 28])\n",
      "the size of the weight of kernels is: torch.Size([3, 1, 3, 3])\n",
      "the weight of kernels is: Parameter containing:\n",
      "tensor([[[[-0.1022,  0.0247, -0.2261],\n",
      "          [ 0.1162,  0.2324, -0.3053],\n",
      "          [ 0.0333, -0.1843, -0.2201]]],\n",
      "\n",
      "\n",
      "        [[[-0.3214, -0.2643,  0.2004],\n",
      "          [-0.0039,  0.1681, -0.1895],\n",
      "          [ 0.0978, -0.0217, -0.1288]]],\n",
      "\n",
      "\n",
      "        [[[-0.1554, -0.1006,  0.0530],\n",
      "          [-0.2089,  0.0737, -0.1504],\n",
      "          [-0.0332,  0.2198, -0.2100]]]], requires_grad=True)\n",
      "the size of the bias of kernels is: torch.Size([3])\n",
      "the bias of kernels is: Parameter containing:\n",
      "tensor([ 0.0915,  0.2550, -0.0520], requires_grad=True)\n",
      "the size of output without padding is: torch.Size([1, 3, 26, 26])\n",
      "--------------------------------------------------------------\n",
      "the size of input is: torch.Size([1, 1, 28, 28])\n",
      "the size of the weight of kernels is: torch.Size([3, 1, 3, 3])\n",
      "the size of the bias of kernels is: torch.Size([3])\n",
      "the size of output with padding is: torch.Size([1, 3, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "print('--------------------------------------------------------------')\n",
    "print('torch.nn.Conv2d()')\n",
    "# torch.nn offer a serial of class inherited from torch.nn.Module\n",
    "# to create 2d-CNN layer by class\n",
    "# create instances from class and input data, the method '__call__' will call the forward() method\n",
    "# to do the forward calculations\n",
    "\n",
    "# create input of a pic batch with 1 pic, 1 channel, 28x28 pixels\n",
    "# i.e. [pic_num, input_channel, input_height, input_width]\n",
    "input = torch.randn(1,1,28,28)\n",
    "# create a layer\n",
    "# paras:\n",
    "# in_channels (int) – Number of channels in the input image, kernel should have same depths/channels\n",
    "# out_channels (int) – Number of channels produced by the convolution, i.e. number of kernels\n",
    "# kernel_size (int or tuple) – Size of the convolving kernel\n",
    "# stride (int or tuple, optional) – Stride of the convolution. Default: 1\n",
    "# padding (int, tuple or str, optional) – Padding added to all four sides of the input. Default: 0\n",
    "#    0 for without padding, 1 for with padding of width 1, and so on\n",
    "# attention:\n",
    "#    a single int:\n",
    "#        used for both the height and width dimension\n",
    "#    a tuple of two ints:\n",
    "#        1st int: the height dimension, 2nd int: the width dimension\n",
    "# the size of filter is: [kernel_num, each_kernel_channel, kernel_height, kernel_width] = [3,1,3,3]\n",
    "# input_channel = each_kernel_channel\n",
    "layer1 = nn.Conv2d(in_channels = 1, out_channels = 3, kernel_size = 3, stride = 1, padding = 0)\n",
    "# output is of:[1,3,26,26], i.e. [pic_num, kernel_num, output_height, output_width]\n",
    "# give input to instace of nn.Conv2D() and directly return forward calculation results\n",
    "output1  = layer1(input)\n",
    "print('the size of input is:', input.size())\n",
    "print('the size of the weight of kernels is:', layer1.weight.size())\n",
    "# the weight has been defautly set 'requires_grad = True'\n",
    "print('the weight of kernels is:', layer1.weight)\n",
    "print('the size of the bias of kernels is:', layer1.bias.size())\n",
    "# the bias has been defautly set 'requires_grad = True'\n",
    "print('the bias of kernels is:', layer1.bias)\n",
    "print('the size of output without padding is:', output1.size())\n",
    "print('--------------------------------------------------------------')\n",
    "layer2 = nn.Conv2d(in_channels = 1, out_channels = 3, kernel_size = 3,stride = 2, padding = 1)\n",
    "output2 = layer2(input)\n",
    "print('the size of input is:', input.size())\n",
    "print('the size of the weight of kernels is:', layer2.weight.size())\n",
    "print('the size of the bias of kernels is:', layer2.bias.size())\n",
    "print('the size of output with padding is:', output2.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06113271",
   "metadata": {},
   "source": [
    "#### torch.nn.functinoal.conv2d()\n",
    "create 2d-CNN layer by function: torch.nn.functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb4a8059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------\n",
      "torch.nn.functional.conv2d()\n",
      "the size of input is: torch.Size([1, 3, 28, 28])\n",
      "the size of weight of kernels is: torch.Size([4, 3, 5, 5])\n",
      "the size of bias of kernels is: torch.Size([4])\n",
      "the size of output is: torch.Size([1, 4, 26, 26])\n"
     ]
    }
   ],
   "source": [
    "print('--------------------------------------------------------------')\n",
    "print('torch.nn.functional.conv2d()')\n",
    "# torch.nn.Functional offer functions directly for CNN calculation\n",
    "# but need to define weight and bias individually\n",
    "\n",
    "# create input of a pic batch with 1 pic, 3 channels, 28x28 pixels\n",
    "# i.e. [pic_num, input_channel, input_height, input_width]\n",
    "input = torch.randn(1,3,28,28)\n",
    "# we want to use 4 kernels to convolute the input\n",
    "# e.g. edge detecting kernel, gaussian blur kernel, looking-for-eyes kernel, looking-for-circle kernel\n",
    "# each of them should have the same depths/channels with input, whcih is 1 here\n",
    "# size of kernels are chosen as 3x3\n",
    "# attention:\n",
    "#    as convention, seld-defined weight tensor has the paras as:\n",
    "#        [channel_out, channel_in, kernel_height, kernel_width]\n",
    "#    which is equal to:\n",
    "#        [kernel_num, input_channel, kernel_height, kernel_width]\n",
    "w = torch.randn(4,3,5,5)\n",
    "# bias should be unidimensional and has the same length with weight's channel_out\n",
    "b = torch.randn(4)\n",
    "output = F.conv2d(input = input, weight = w, bias = b, stride = 1, padding = 1)\n",
    "print('the size of input is:', input.size())\n",
    "print('the size of weight of kernels is:', w.size())\n",
    "print('the size of bias of kernels is:', b.size())\n",
    "print('the size of output is:', output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909fa2e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
