{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8155008d",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9ac3057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e376c95a",
   "metadata": {},
   "source": [
    "#### Image Normalization\n",
    "<font size=2>\n",
    "\n",
    "In order to avoid of gradient divergency (e.g. from Sigmoid) when datas' output are too close to the realm of plateau, which is shown below, normalization should be applied.\n",
    "    \n",
    "<div>\n",
    "<img src = \"SigmoidPlateau.png\" style = \"zoom: 60%\" />\n",
    "</div>\n",
    "    \n",
    "The formular for normalization is following, which is aimed at transferring data into a united distribution of $N(0,1)$:\n",
    "    \n",
    "$$ \\tilde{d} = \\frac{d - \\mu}{\\sigma} $$\n",
    "    \n",
    "where $d$ is original data, $\\tilde{d}$ is normalized data, $\\mu$ is statistical mean of original data, $\\sigma$ is statistical standard variance of original data.\n",
    "    \n",
    "Attention: $\\mu$ and $\\sigma$ are from specific calculation from original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f753483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.6010, -0.0446],\n",
      "          [ 1.5668,  0.4178]],\n",
      "\n",
      "         [[ 0.8321,  0.2740],\n",
      "          [-1.9274,  1.1882]],\n",
      "\n",
      "         [[-1.9613,  2.0621],\n",
      "          [ 0.5819,  0.1401]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8352,  0.1644],\n",
      "          [-1.5332, -0.4747]],\n",
      "\n",
      "         [[ 1.9882, -1.1932],\n",
      "          [ 1.7495, -0.4430]],\n",
      "\n",
      "         [[ 1.6050,  1.6737],\n",
      "          [ 0.3339, -0.8236]]]])\n",
      "tensor([[[[ 0.8679,  1.5408],\n",
      "          [ 0.8657, -0.6081]],\n",
      "\n",
      "         [[-0.2586, -0.6329],\n",
      "          [ 1.2518, -0.3239]],\n",
      "\n",
      "         [[ 0.0646,  1.5763],\n",
      "          [ 1.4138,  1.9536]]],\n",
      "\n",
      "\n",
      "        [[[-1.8601, -1.4089],\n",
      "          [ 0.7828,  0.1823]],\n",
      "\n",
      "         [[ 1.3136,  1.1332],\n",
      "          [ 1.8170, -1.8920]],\n",
      "\n",
      "         [[-0.7052, -1.6217],\n",
      "          [ 1.0318,  1.0576]]]])\n"
     ]
    }
   ],
   "source": [
    "# image dataset with 2 images, 3 channels, shape of 2x2 pixels\n",
    "img_data = torch.rand(2,3,2,2)\n",
    "print(out)\n",
    "# the 'mean' and 'std' are mean and standard variance separately of 3 channels\n",
    "# which are statistics from image dataset\n",
    "# in order to normalize input data into a distribution of N(0,1)\n",
    "normalizer = transforms.Normalize(mean=[0.45,0.52,0.48],std=[0.23,0.236,0.24])\n",
    "out = normalizer(img_data)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987c7a64",
   "metadata": {},
   "source": [
    "#### Batch Normalization\n",
    "<font size=2>\n",
    "    \n",
    "In one batch deploy normalization along with the dimension of channels. For example: a batch of 5 images with 16 channels, 28x28 pixels: **[5,16,28,28]**, or flattened version: **[5,16,784]**. The normalizatio is along with the dimension with size of **16**(i.e. channel dimension), so there will be 16 $\\mu$ and 16 $\\sigma$ for each channel.\n",
    "    \n",
    "The following image illustrates vividly how to normalize datasets:\n",
    "    \n",
    "<div>\n",
    "<img src = \"BatchNorm.png\" style = \"zoom:50%\" />\n",
    "</div>\n",
    "    \n",
    "The $\\mu$ and $\\sigma$ are calculated from original data for each channel. Lets say $\\mu_{1}$ and $\\sigma_{1}$ are mean and standard variance for **channel 1**, which are calculated from output $z^{1}$. And $\\tilde{z}^{1}$ is normalized $z^{1}$ by:\n",
    "    \n",
    "$$ \\tilde{z}^{1} = \\frac{z^{1} - \\mu_{1}}{\\sigma_{1}} $$\n",
    "    \n",
    "When we still want the **scale** and **shift** data into a new distribution $N(\\beta,\\gamma)$, where $\\beta$ and $\\gamma$ are parameters in the network, that is to say that they are **learned**:\n",
    "    \n",
    "$$ \\hat{z}^{1} = \\gamma \\odot \\tilde{z}^{1} + \\beta $$\n",
    "    \n",
    "And there is the formular pipeline for batch normalization:\n",
    "    \n",
    "<div>\n",
    "<img src = \"BatchNormPipeline.png\" style = \"zoom:50%\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0776153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer.running_mean: tensor([0.0498, 0.0500, 0.0498, 0.0500, 0.0501, 0.0498, 0.0500, 0.0501, 0.0499,\n",
      "        0.0500, 0.0501, 0.0499, 0.0500, 0.0500, 0.0499, 0.0500]) with shape of: torch.Size([16])\n",
      "layer.running_var: tensor([0.9083, 0.9084, 0.9083, 0.9084, 0.9084, 0.9083, 0.9083, 0.9084, 0.9083,\n",
      "        0.9083, 0.9083, 0.9084, 0.9084, 0.9083, 0.9083, 0.9084]) with shape of: torch.Size([16])\n",
      "layer.weight: Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       requires_grad=True) with shape of: torch.Size([16])\n",
      "layer.bias: Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True) with shape of: torch.Size([16])\n",
      "----------------------------------------------------------------------------------------------\n",
      "{'training': True, '_parameters': OrderedDict([('weight', Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       requires_grad=True)), ('bias', Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True))]), '_buffers': OrderedDict([('running_mean', tensor([0.0498, 0.0500, 0.0498, 0.0500, 0.0501, 0.0498, 0.0500, 0.0501, 0.0499,\n",
      "        0.0500, 0.0501, 0.0499, 0.0500, 0.0500, 0.0499, 0.0500])), ('running_var', tensor([0.9083, 0.9084, 0.9083, 0.9084, 0.9084, 0.9083, 0.9083, 0.9084, 0.9083,\n",
      "        0.9083, 0.9083, 0.9084, 0.9084, 0.9083, 0.9083, 0.9084])), ('num_batches_tracked', tensor(1))]), '_non_persistent_buffers_set': set(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_load_state_dict_post_hooks': OrderedDict(), '_modules': OrderedDict(), 'num_features': 16, 'eps': 1e-05, 'momentum': 0.1, 'affine': True, 'track_running_stats': True}\n"
     ]
    }
   ],
   "source": [
    "# a batch of 100 images, 16 channels, 28x28 pixels but flattened\n",
    "data_1d = torch.rand(100,16,784)\n",
    "# data_2d = torch.rand(100,16,28,28)  # 2d version\n",
    "# input of nn.BatchNorm1d() is the channel size of input data\n",
    "layer_1d = nn.BatchNorm1d(16)\n",
    "# layer_2d = nn.BatchNorm2d(16)  # 2d version\n",
    "out = layer_1d(data_1d)\n",
    "# running_mean & running_var:\n",
    "# the dynamicly updated mean 'mu' and std 'sigma' of this batch\n",
    "print('layer.running_mean: {} with shape of: {}'.format(layer_1d.running_mean, layer_1d.running_mean.shape))\n",
    "print('layer.running_var: {} with shape of: {}'.format(layer_1d.running_var, layer_1d.running_var.shape))\n",
    "# weight & bias\n",
    "# the learned mean 'beta' and learned std 'gamma', which need gradients to update\n",
    "print('layer.weight: {} with shape of: {}'.format(layer_1d.weight, layer_1d.weight.shape))\n",
    "print('layer.bias: {} with shape of: {}'.format(layer_1d.bias, layer_1d.bias.shape))\n",
    "print('----------------------------------------------------------------------------------------------')\n",
    "# all parameters of current layer\n",
    "print(vars(layer_1d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a710de1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
