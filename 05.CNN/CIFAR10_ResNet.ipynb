{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3814dad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "#pyhon升级到2.7.9以后，引入了一个新特性，当使用urllib打开https的链接时，\n",
    "#会检验一次ssl证书。而当目标网站使用的是自签名证书时，就会抛出urllib2.URLError的错误\n",
    "#全局取消证书验证可解决\n",
    "#CIFAR10数据集下载地址出了问题\n",
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2837ab18",
   "metadata": {},
   "source": [
    "#### CIFAR10 Dataset\n",
    "<font size = 2>\n",
    "\n",
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "\n",
    "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
    "    \n",
    "Here are the classes in the dataset, as well as 10 random images from each:\n",
    "    \n",
    "<div>\n",
    "<img src = \"CIFAR10_1.png\" style = \"zoom:70%\" />\n",
    "</div>\n",
    "  \n",
    "The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. \"Automobile\" includes sedans, SUVs, things of that sort. \"Truck\" includes only big trucks. Neither includes pickup trucks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05eadb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare CIFAR10 dataset\n",
    "def get_loaders(BatchSize):\n",
    "    #get train loader\n",
    "    CIFAR_train = DataLoader(\n",
    "        datasets.CIFAR10(\n",
    "            root = '/home/hardli/python/pytorch/datasets',\n",
    "            train = True,\n",
    "            download = True,\n",
    "            transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((32,32))\n",
    "            ])\n",
    "        ),\n",
    "        batch_size = BatchSize,\n",
    "        shuffle = True\n",
    "    )\n",
    "    #get test loader\n",
    "    CIFAR_test = DataLoader(\n",
    "        datasets.CIFAR10(\n",
    "            root = '/home/hardli/python/pytorch/datasets',\n",
    "            train = False,\n",
    "            download = True,\n",
    "            transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((32,32))\n",
    "            ])\n",
    "        ),\n",
    "        batch_size = BatchSize,\n",
    "        shuffle = True\n",
    "    )\n",
    "\n",
    "    return CIFAR_train, CIFAR_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3db63a2",
   "metadata": {},
   "source": [
    "#### Data in CIFAR10\n",
    "<font size = 2>\n",
    "    \n",
    "The data after dataloader operation in CIFAR10 comprises of lists. Single list contain 2 elements. One is image data with shape of [3,32,32], the other is label which is unidimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd38378c",
   "metadata": {},
   "source": [
    "#### LeNet\n",
    "<font size = 2>\n",
    "\n",
    "LeNet has the structure as below. However, we adjust a little on original structure. Change the **Sub-sampling layers** into **Pooling Layers**:\n",
    "    \n",
    "<div>\n",
    "<img src = \"LeNet.png\" style = \"zoom:70%\" />\n",
    "</div>\n",
    "\n",
    "Input are CIFAR10 data with 32x32 pixels for each image.\n",
    "    \n",
    "And the hidden layers are arranged as:\n",
    "    \n",
    "    1. Convolutional layer with 6 5x5 kernels, stride 1, no padding;\n",
    "    2. Pooling layer with 2x2 windows, stride 2, no padding;\n",
    "    3. Convolutional layer with 16 5x5 kernels, stride 1, no padding;\n",
    "    4. Pooling layer with 2x2 window, stride 2, no padding;\n",
    "    5. Linear fully connected layers;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39377159",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create LeNet class\n",
    "class LeNet5(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(LeNet5, self).__init__()\n",
    "        #convolutional layers\n",
    "        self.conv_unit = nn.Sequential(\n",
    "            #original date is of RGB 3 channel\n",
    "            #shape: [b,3,32,32] -> [b,6,28,28]\n",
    "            nn.Conv2d(in_channels = 3,out_channels = 6,kernel_size = 5,stride = 1,padding = 0),\n",
    "            #shape: [b,6,28,28] -> [b,6,14,14]\n",
    "            nn.AvgPool2d(kernel_size = 2,stride = 2,padding = 0),\n",
    "            #shape: [b,6,14,14] -> [b,16,10,10]\n",
    "            nn.Conv2d(in_channels = 6,out_channels = 16,kernel_size = 5,stride = 1,padding = 0),\n",
    "            #shape: [b,16,10,10] -> [b,16,5,5]\n",
    "            nn.AvgPool2d(kernel_size = 2,stride = 2,padding = 0)\n",
    "        )\n",
    "        #flatten layer\n",
    "        \n",
    "        #linear fully connected layers\n",
    "        self.FC_unit = nn.Sequential(\n",
    "            #channel change: 16*5*5 -> 120\n",
    "            #16*5*5 comes from flatten batch\n",
    "            #see forward()\n",
    "            nn.Linear(16*5*5, 120),\n",
    "            nn.ReLU(),\n",
    "            #channel change: 120 -> 84\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            #channel change: 84 -> 10\n",
    "            nn.Linear(84, 10)\n",
    "        )\n",
    "        \n",
    "#         #try\n",
    "#         tmp = torch.randn(2,3,32,32)\n",
    "#         out = self.conv_unit(tmp)\n",
    "#         print('out size:', out.size())\n",
    "\n",
    "    def forward(self,x):\n",
    "        #[b,3,32,32], b is batch siez, i.e. numbers of pictures in a single batch\n",
    "        batch_size = x.size(0)\n",
    "        #convolutional operation\n",
    "        #[b,3,32,32] -> [b,16,5,5]\n",
    "        x = self.conv_unit(x)\n",
    "        #flatten operation\n",
    "        #[b,16,5,5] -> [b,16*5*5]\n",
    "        x = x.view(batch_size, -1)\n",
    "        #fully connected operation\n",
    "        #[b,16*5*5] -> [b,10]\n",
    "        logits = self.FC_unit(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec64948",
   "metadata": {},
   "source": [
    "#### ResNet\n",
    "<font size = 2>\n",
    "\n",
    "ResNet has **skipping/shortcut** part which make the input skip the **layer block** and added into result of hidden layers of **layer block**:\n",
    "    \n",
    "<div>\n",
    "<img src = \"ResNet3.png\" style = \"zoom:70%\" />\n",
    "</div>\n",
    "\n",
    "Input are CIFAR10 data with [3,32,32] pixels for each image.\n",
    "    \n",
    "The **weight layer** here are assigned as **convolutional layers**.\n",
    "    \n",
    "$F(x)$ is result after calculating of hidden layers, $x$ is input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61df6b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create class ResNet_block: one block with skipping/shortcut unit\n",
    "class ResNet_block(nn.Module):\n",
    "    \n",
    "    def __init__(self, ch_in, ch_out, stride = 1):\n",
    "        \n",
    "        '''\n",
    "        param ch_in:\n",
    "        param ch_out:\n",
    "        '''\n",
    "        \n",
    "        super(ResNet_block, self).__init__()\n",
    "        #two convolutional layers as shown above\n",
    "        #change channels\n",
    "        #change pic scale [h,w]\n",
    "        self.conv1 = nn.Conv2d(ch_in, ch_out, kernel_size = 3, stride = stride, padding = 1)\n",
    "        self.bn1 = nn.BatchNorm2d(ch_out)\n",
    "        self.conv2 = nn.Conv2d(ch_out, ch_out, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.bn2 = nn.BatchNorm2d(ch_out)\n",
    "        \n",
    "        #in case ch_in is not equal to ch_out\n",
    "        #[b,ch_in,h,w] -> [b,ch_out,h,w]\n",
    "        self.extra = nn.Sequential()\n",
    "        if ch_in != ch_out:\n",
    "            self.extra = nn.Sequential(\n",
    "                #this operation is to resize the channel of input as result after block\n",
    "                #self.conv1 makes: [b,c_in,h,w] -> [b,c_out,h/stride,w/stride](approximately)\n",
    "                #with kernel_size = 3 and padding = 1\n",
    "                #here we want to get the same dimensional shape\n",
    "                #and with kernel_size = 1\n",
    "                #so set padding = 0\n",
    "                nn.Conv2d(ch_in, ch_out, kernel_size = 1, stride = stride, padding = 0),\n",
    "                nn.BatchNorm2d(ch_out)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        '''\n",
    "        param x: [b,ch_in,h,w]\n",
    "        return:\n",
    "        '''\n",
    "        \n",
    "        #we need to skip the block, so original input x should be kept\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = (self.bn2(self.conv2(out)))\n",
    "        #skipping/shortcut\n",
    "        #element-wise addition: [b,ch_in,h,w] + [b,ch_out,h,w]\n",
    "        #in case ch_in is not equal to ch_out\n",
    "        out = out + self.extra(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b8773de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a Residual Network\n",
    "class ResNet18(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(ResNet18,self).__init__()\n",
    "        \n",
    "        #first process:[b,3,h,w] -> [b,64,h,w]\n",
    "        self.first_conv = nn.Sequential(\n",
    "            nn.Conv2d(3,64,kernel_size = 3,stride = 3,padding = 0),\n",
    "            nn.BatchNorm2d(64)\n",
    "            )\n",
    "        \n",
    "        #block1: [b,64,h,w] -> [b,128,h,w]\n",
    "        self.block1 = ResNet_block(64,128,stride = 2)\n",
    "        #block1: [b,128,h,w] -> [b,256,h,w]\n",
    "        self.block2 = ResNet_block(128,256,stride = 2)\n",
    "        #block1: [b,256,h,w] -> [b,512,h,w]\n",
    "        self.block3 = ResNet_block(256,512,stride = 2)\n",
    "        #block1: [b,512,h,w] -> [b,512,h,w]\n",
    "        self.block4 = ResNet_block(512,512,stride = 2)\n",
    "        \n",
    "        #out layer\n",
    "        self.outlayer = nn.Linear(512*1*1, 10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #[b,3,h,w] -> [b,64,h,w]\n",
    "        x = F.relu(self.first_conv(x))\n",
    "        #[b,64,h,w] -> [b,1024,h,w]\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "#         print('after conv:', x.shape)\n",
    "        #reshape\n",
    "        #F.adaptive_avg_pool2d(x,[1,1]) can do adaptive pooling\n",
    "        #change input x with whatever scale [h,w] into designated shape, here is [1,1]\n",
    "        #a.k.a, [b,c,h,w] -> [b,c,1,1]\n",
    "        x = F.adaptive_avg_pool2d(x,[1,1])\n",
    "#         print('after pooling:', x.shape)\n",
    "        #change shape of x: [b,c,1,1] -> [b,c*1*1]\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.outlayer(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "675c8816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "\n",
    "    BatchSize = 3000\n",
    "\n",
    "    CIFAR_train, CIFAR_test = get_loaders(BatchSize)\n",
    "    \n",
    "    #create gpu device\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    #create a network model\n",
    "    model_name = int(input('Select a model: (1)LeNet5 or (2)ResNet?'))\n",
    "    if model_name == 1:\n",
    "        model = LeNet5().to(device)\n",
    "    elif model_name == 2:\n",
    "        model = ResNet18().to(device)\n",
    "    #create loss function\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    #create a optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "    #show model details\n",
    "    print(model)\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        \n",
    "        #train mode\n",
    "        model.train()\n",
    "        for batch_idx,(img,label) in enumerate(CIFAR_train):\n",
    "            #img:   [b,3,32,32]\n",
    "            #label: [b]\n",
    "            img, label = img.to(device), label.to(device)\n",
    "            #logits: [b,10]\n",
    "            logits = model(img)\n",
    "            #loss: [b,10]\n",
    "            #nn.CrossEntropyLoss() includes operation of softmax\n",
    "            loss = criterion(logits, label)\n",
    "            \n",
    "            #backpropagation\n",
    "            #clean gradients, or gradients will accmulate\n",
    "            optimizer.zero_grad()\n",
    "            #get gradients of parameters\n",
    "            loss.backward()\n",
    "            #update gradients of parameters\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'loss of {epoch}th epoch: {loss}')\n",
    "        \n",
    "        #test mode\n",
    "        model.eval()\n",
    "        #due to that testing has no need to calculate gradients, it is unnecessary to create graph\n",
    "        #put the testing coding under 'torch.no_grad()' to avoid troubles\n",
    "        with torch.no_grad():\n",
    "\n",
    "            #count the correct classifications\n",
    "            total_correct = 0\n",
    "            #count the total number of testing pictures\n",
    "            total_num = 0\n",
    "            #accuracy of testing\n",
    "            acc = 0\n",
    "\n",
    "            for batch_idx,(img,label) in enumerate(CIFAR_test):\n",
    "                #img:   [b,3,32,32]\n",
    "                #label: [b]\n",
    "                img, label = img.to(device), label.to(device)\n",
    "                #logits:[b,10]\n",
    "                #'10' means 10 classes that the pictures may be labeled into\n",
    "                #each picture in current batch is of [1,10]\n",
    "                #the 10 values can be considered as possibility of pic in corresponding class\n",
    "                #so the largest value means the highest possibility\n",
    "                #which is considered as the predictive label\n",
    "                logits = model(img)\n",
    "                #preds:  [b]\n",
    "                #take index of maximum along the 1st dimension, i.e. values dimension\n",
    "                preds = logits.argmax(dim = 1)\n",
    "                #total_correct: sclar\n",
    "                #count how many predictive labels are equal to target labels\n",
    "                total_correct += torch.eq(preds, label).float().sum().item()\n",
    "                #total_num: scalar\n",
    "                #because img is of [b,3,32,32], img.size(0) = b\n",
    "                #which is the number of pictures in a single batch\n",
    "                total_num += img.size(0)\n",
    "            \n",
    "            #acc: scalar    \n",
    "            acc = total_correct / total_num\n",
    "            print(f'accuracy of epoch {epoch} is: {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69ab5690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Select a model: (1)LeNet5 or (2)ResNet?2\n",
      "ResNet18(\n",
      "  (first_conv): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(3, 3))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (block1): ResNet_block(\n",
      "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (extra): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (block2): ResNet_block(\n",
      "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (extra): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (block3): ResNet_block(\n",
      "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (extra): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (block4): ResNet_block(\n",
      "    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (extra): Sequential()\n",
      "  )\n",
      "  (outlayer): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "loss of 0th epoch: 1.6014410257339478\n",
      "accuracy of epoch 0 is: 0.1767\n",
      "loss of 1th epoch: 1.3189243078231812\n",
      "accuracy of epoch 1 is: 0.3126\n",
      "loss of 2th epoch: 1.1463193893432617\n",
      "accuracy of epoch 2 is: 0.4501\n",
      "loss of 3th epoch: 0.9701715707778931\n",
      "accuracy of epoch 3 is: 0.5706\n",
      "loss of 4th epoch: 0.870572030544281\n",
      "accuracy of epoch 4 is: 0.5966\n",
      "loss of 5th epoch: 0.7459280490875244\n",
      "accuracy of epoch 5 is: 0.6079\n",
      "loss of 6th epoch: 0.6167778372764587\n",
      "accuracy of epoch 6 is: 0.609\n",
      "loss of 7th epoch: 0.48955056071281433\n",
      "accuracy of epoch 7 is: 0.6219\n",
      "loss of 8th epoch: 0.37858766317367554\n",
      "accuracy of epoch 8 is: 0.6227\n",
      "loss of 9th epoch: 0.28195062279701233\n",
      "accuracy of epoch 9 is: 0.61\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a99e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
