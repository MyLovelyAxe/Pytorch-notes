{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a80a7a15",
   "metadata": {},
   "source": [
    "### Math Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e387360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f79d6b1",
   "metadata": {},
   "source": [
    "#### Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "858754d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "addition: \n",
      "tensor([[1, 6],\n",
      "        [6, 3],\n",
      "        [4, 3]])\n",
      "tensor([2, 2])\n",
      "tensor([[3, 8],\n",
      "        [8, 5],\n",
      "        [6, 5]])\n",
      "a+b == torch.add(a,b):  tensor(True)\n",
      "----------------------------------------------------------\n",
      "substraction: \n",
      "a-b == torch.sub(a,b):  tensor(True)\n",
      "----------------------------------------------------------\n",
      "elementary multiplication: \n",
      "a*b == torch.mul(a,b):  tensor(True)\n",
      "torch.Size([3, 2])\n",
      "----------------------------------------------------------\n",
      "division: \n",
      "a/b == torch.div(a,b):  tensor(True)\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "a = torch.randint(0,10,(3,2))\n",
    "b = torch.full([2],2)\n",
    "# addition\n",
    "print('addition: ')\n",
    "# take addition as example:\n",
    "# broadcasting of torch.tensor is similar with numpy.array\n",
    "# i.e. lower-dim tensor broadcasts to higher-dim tensor along with the hightest dimension\n",
    "# e.g. a.shape=(3,2), b.shape=(2,), broadcasting works\n",
    "# however, if a.shape=(3,2), b.shape=(3,), broadcasting doesn't work\n",
    "print(a)\n",
    "print(b)\n",
    "print(a+b)\n",
    "print('a+b == torch.add(a,b): ', torch.all(torch.eq(a+b,torch.add(a,b))))\n",
    "print('----------------------------------------------------------')\n",
    "# substraction\n",
    "print('substraction: ')\n",
    "print('a-b == torch.sub(a,b): ', torch.all(torch.eq(a-b,torch.sub(a,b))))\n",
    "print('----------------------------------------------------------')\n",
    "# elementary multiplication\n",
    "# attention: this is different from matrix multiplication\n",
    "print('elementary multiplication: ')\n",
    "print('a*b == torch.mul(a,b): ', torch.all(torch.eq(a*b,torch.mul(a,b))))\n",
    "print((a*b).shape)\n",
    "print('----------------------------------------------------------')\n",
    "# division\n",
    "print('division: ')\n",
    "print('a/b == torch.div(a,b): ', torch.all(torch.eq(a/b,torch.div(a,b))))\n",
    "print('----------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3cea3c",
   "metadata": {},
   "source": [
    "#### Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "10f977df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: \n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]]) torch.Size([2, 3]) torch.FloatTensor\n",
      "b: \n",
      "tensor([1., 1., 1.]) torch.Size([3]) torch.FloatTensor\n",
      "a@b == torch.matmul(a,b):  tensor(True)\n",
      "-------------------------------------------------------------------------\n",
      "inp shape:  torch.Size([4, 784])\n",
      "inp @ weight.t():  torch.Size([4, 512])\n",
      "-------------------------------------------------------------------------\n",
      "pic_batch @ layer:  torch.Size([4, 3, 32, 14])\n"
     ]
    }
   ],
   "source": [
    "a = torch.full([2,3],2.)\n",
    "print('a: ')\n",
    "print(a, a.shape, a.type())\n",
    "b = torch.ones(3)\n",
    "print('b: ')\n",
    "print(b, b.shape, b.type())\n",
    "# datatypes of multipliers must be same\n",
    "print('a@b == torch.matmul(a,b): ', torch.all(torch.eq(a@b,torch.matmul(a,b))))\n",
    "\n",
    "'''example of dimension decreasing'''\n",
    "print('-------------------------------------------------------------------------')\n",
    "# flatten a dataset with 4 pictures, 3 channels, 28x28 pixels into [4,784]\n",
    "# decrease dimesnion through layer1 from [4,748] to [4,512]\n",
    "inp = torch.randn(4,784)\n",
    "print('inp shape: ', inp.shape)\n",
    "# in convention of Pytorch, weight should have shpe of:[channel_out,channel_in]\n",
    "# and take transpose when applied\n",
    "weight = torch.randn(512,784)\n",
    "output = inp@weight.t()\n",
    "print('inp @ weight.t(): ', output.shape)\n",
    "\n",
    "'''multiple matrix-multiplication in parallel'''\n",
    "print('-------------------------------------------------------------------------')\n",
    "# only last 2 dimensions will excute matrix multiplication\n",
    "pic_batch = torch.randn([4,3,32,28])\n",
    "layer = torch.randn([4,3,28,14])\n",
    "output = torch.matmul(pic_batch,layer)\n",
    "print('pic_batch @ layer: ', output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db59ce24",
   "metadata": {},
   "source": [
    "#### dimension details of matrix multiplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4efd4fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:  torch.Size([3])\n",
      "d:  torch.Size([2, 3])\n",
      "d x c:  torch.Size([2])\n",
      "--------------------------------------------\n",
      "c.t()==c:  tensor(True)\n",
      "c.t():  torch.Size([3])\n",
      "d:  torch.Size([2, 3])\n",
      "d x c.t():  torch.Size([2])\n",
      "--------------------------------------------\n",
      "c:  torch.Size([3])\n",
      "d.T:  torch.Size([3, 2])\n",
      "c x d.T:  torch.Size([2])\n",
      "--------------------------------------------\n",
      "c:  torch.Size([3, 1])\n",
      "d:  torch.Size([2, 3])\n",
      "d x c:  torch.Size([2, 1])\n",
      "--------------------------------------------\n",
      "c:  torch.Size([1, 3])\n",
      "d.T:  torch.Size([3, 2])\n",
      "c x d.T:  torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "c = torch.rand(3)\n",
    "d = torch.rand(2,3)\n",
    "\n",
    "# c.shape: [3]\n",
    "# d.shape: [2,3]\n",
    "# if c x d: [3] x [2,3], c must be automatically reshape as [smth,2], which is impossible\n",
    "# so c x d doesn't work\n",
    "# if d x c: [2,3] x [3], c can be considered as vertical vector,\n",
    "# so the result is [2], which has same dimension with c\n",
    "print('c: ', c.shape)\n",
    "print('d: ', d.shape)\n",
    "print('d x c: ', torch.matmul(d,c).shape)\n",
    "print('--------------------------------------------')\n",
    "\n",
    "# transpose of 1D tensor is the same as itself\n",
    "# the rule is also the same with previous situation\n",
    "print('c.t()==c: ', torch.all(c.t()==c))\n",
    "print('c.t(): ', (c.t()).shape)\n",
    "print('d: ', d.shape)\n",
    "print('d x c.t(): ', torch.matmul(d,c.T).shape)\n",
    "print('--------------------------------------------')\n",
    "\n",
    "# if c x d.T: [3] x [3,2], c can be considered as horizontal vector,\n",
    "# so the result is also [2], which has same dimension with c\n",
    "# if d.T x c: [3,2] x [3], c must be automatically reshape as [2,smth], which is impossible\n",
    "# so d.T x c doesn't work\n",
    "print('c: ', c.shape)\n",
    "print('d.T: ', (d.T).shape)\n",
    "print('c x d.T: ', torch.matmul(c, d.T).shape)\n",
    "print('--------------------------------------------')\n",
    "\n",
    "# if reshape c as [3,1] or [1,3], the situation will be same with normal matrix multiplication\n",
    "# and pay attention to corresponding dimenions\n",
    "c = c.reshape(3,1)\n",
    "print('c: ', c.shape)\n",
    "print('d: ', d.shape)\n",
    "print('d x c: ', torch.matmul(d,c).shape)\n",
    "print('--------------------------------------------')\n",
    "\n",
    "c = c.reshape(1,3)\n",
    "print('c: ', c.shape)\n",
    "print('d.T: ', (d.T).shape)\n",
    "print('c x d.T: ', torch.matmul(c,d.T).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e996fa6a",
   "metadata": {},
   "source": [
    "#### Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1fb6be20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 4],\n",
      "        [4, 4]])\n",
      "tensor([[16, 16],\n",
      "        [16, 16]])\n",
      "tensor([[16, 16],\n",
      "        [16, 16]])\n",
      "tensor([[16, 16],\n",
      "        [16, 16]])\n",
      "tensor([[16, 16],\n",
      "        [16, 16]])\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.full([2,2],4)\n",
    "print(a)\n",
    "print(a**2)\n",
    "print(a.pow(2))\n",
    "print(torch.pow(a,2))\n",
    "print(pow(a,2))\n",
    "print(a.sqrt())\n",
    "print(a**(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3f78e8",
   "metadata": {},
   "source": [
    "#### Exp and Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cfbbbe88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:  tensor([[2.7183, 2.7183],\n",
      "        [2.7183, 2.7183]])\n",
      "log_e tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "log_10 tensor([[0.4343, 0.4343],\n",
      "        [0.4343, 0.4343]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2,2)\n",
    "e = torch.exp(a)\n",
    "print('e: ', e)\n",
    "# torch.log() takes 'e' as basis defaultly\n",
    "log_e = torch.log(e)\n",
    "print('log_e', log_e)\n",
    "# torch.log10() takes 10 as basis, and so on\n",
    "log_10 = torch.log10(e)\n",
    "print('log_10', log_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c088d93",
   "metadata": {},
   "source": [
    "#### Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1415278e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.) tensor(4.) tensor(3.) tensor(0.4000)\n",
      "tensor(4.)\n",
      "tensor(3.)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(3.4)\n",
    "# .floor():  take upper integer\n",
    "# .ceil():   take lower integer\n",
    "# .trunc():  take integer part\n",
    "# .frac():   take fraction part\n",
    "print(a.floor(),a.ceil(),a.trunc(),a.frac())\n",
    "# 四舍五入\n",
    "print(torch.tensor(3.5).round())\n",
    "print(torch.tensor(3.49).round())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487850a0",
   "metadata": {},
   "source": [
    "#### Clamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e9f70c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  2.9546, -29.4667, -22.4124],\n",
      "        [  8.9374,   1.9090,  -0.4860]])\n",
      "tensor([[5.0000, 5.0000, 5.0000],\n",
      "        [8.9374, 5.0000, 5.0000]])\n",
      "tensor([[2.9546, 0.0000, 0.0000],\n",
      "        [8.9374, 1.9090, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# e.g. gradient clamping\n",
    "grad = torch.randn(2,3)*15\n",
    "print(grad)\n",
    "# input: .clamp(min)\n",
    "# replace all elements less than 'min' with 'min'\n",
    "# others remain\n",
    "clmp1 = grad.clamp(5)\n",
    "print(clmp1)\n",
    "# input: .clamp(min,max)\n",
    "# replace all elements less than 'min' with 'min'\n",
    "#                      greater than 'max' with 'max'\n",
    "# others remain\n",
    "clmp1 = grad.clamp(0,20)\n",
    "print(clmp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f47a44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
