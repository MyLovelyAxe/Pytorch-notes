{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8141aac9",
   "metadata": {},
   "source": [
    "### Slicing and Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b466bc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004d64ca",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "<font size = 2>\n",
    "    \n",
    "For index of each dimension:  **(Start:End:Step)**    \n",
    "    \n",
    "1.The index starts from **Start**, ends at **End**(excluded), by step of **Step**;\n",
    "    \n",
    "     (Start:End:Step)\n",
    "    \n",
    "2.If **Start** is missing, it means to start from 0th element defautly, by step of **Step**;\n",
    "    \n",
    "    (:End:Step)\n",
    "    \n",
    "3.If **End** is missing, it means to ends at last element(included), by step of **Step**;\n",
    "    \n",
    "    (Start::Step)\n",
    "    \n",
    "4.If both of **Start** and **End** are missing, it means to select all;\n",
    "    \n",
    "    ( : )\n",
    "    \n",
    "5.Only **Step** remainning means select all by step of **Step**;\n",
    "    \n",
    "    (::Step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7dfe0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "picture dataset: torch.Size([16, 3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# create a situation: picture dataset with 16 pics, 3 channels, 28x28 pixels\n",
    "dataset = torch.rand(16,3,28,28)\n",
    "print('picture dataset:', dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c0687da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select some dimensions:\n",
      "pic4,i.e. dataset[4]: torch.Size([3, 28, 28])\n",
      "the channel3 of pic4,i.e. dataset[4,2]: torch.Size([28, 28])\n",
      "one pixel on channe3 of pic4,i.e. dataset[4,2,1,1]: tensor(0.8453)\n"
     ]
    }
   ],
   "source": [
    "'''select some dimensions'''\n",
    "print('select some dimensions:')\n",
    "print('pic4,i.e. dataset[4]:', dataset[4].shape)\n",
    "print('the channel3 of pic4,i.e. dataset[4,2]:', dataset[4,2].shape)\n",
    "print('one pixel on channe3 of pic4,i.e. dataset[4,2,1,1]:', dataset[4,2,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cb2364",
   "metadata": {},
   "source": [
    "### Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a62703a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 28, 28])\n",
      "torch.Size([2, 2, 28, 28])\n",
      "torch.Size([2, 2, 28, 28])\n",
      "torch.Size([2, 1, 28, 28])\n",
      "torch.Size([2, 2, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "'''select first N or last N dimensions'''\n",
    "# take the first 2 pictures\n",
    "# e.g. 0th dim:  starts from 0 defaultly, ends at 2(excluded)\n",
    "b0 = dataset[:2]\n",
    "# take the first 2 channels of first 2 pictures\n",
    "#e.g. 1st dim:  starts from 0 defaultly, ends at 2(excluded)\n",
    "b1 = dataset[:2,:2]\n",
    "# take the last 2 channels of first 2 pictures\n",
    "# e.g. 1st dim:  starts from 1, ends at last(included)\n",
    "# e.g. 2nd and 3rd dim: only ':' means take all\n",
    "b2 = dataset[:2,1:,:,:]\n",
    "# take the last channels of first 2 pictures\n",
    "# ps:\n",
    "# positive index: 0,1,2,...\n",
    "# negative index: -1,-2,-3,...\n",
    "# e.g. 1st dim:  starts from -1(i.e. the last), ends at last, resulting in only 1\n",
    "b3 = dataset[:2,-1:,:,:]\n",
    "# take first 2 channels of first 2 pictures\n",
    "# e.g. 1st dim:  starts from 0 defaultly, ends at -1(i.e. last, and excluded)\n",
    "b4 = dataset[:2,:-1,:,:]\n",
    "\n",
    "b_lst = [b0, b1, b2, b3, b4]\n",
    "for i in b_lst:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebc3a95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 6, 6])\n",
      "torch.Size([16, 3, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "'''select by steps'''\n",
    "# take all pictures, all channels\n",
    "# but parts of each channels\n",
    "# e.g. 2nd dim: starts from 2, ends at 13(excluded), select by step 2 \n",
    "c0 = dataset[:,:,2:13:2,2:13:2]\n",
    "# down-sample with step 2\n",
    "# i.e. starts from 0 defautly, ends at last(included), by step 2\n",
    "c1 = dataset[:,:,::2,::2]\n",
    "\n",
    "c_lst = [c0, c1]\n",
    "for i in c_lst:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1f87dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 28, 28])\n",
      "torch.Size([3, 28, 28])\n",
      "torch.Size([16, 3, 28, 3])\n",
      "torch.Size([16, 3, 28, 3])\n"
     ]
    }
   ],
   "source": [
    "'''select by specific index'''\n",
    "# use .index_select(dim,[idx1,idx2,....])\n",
    "# to select arrays on 'dim' with index 'idx1,idx2,....'\n",
    "# ps:\n",
    "#   the [idx1,idx2,...] should be a tensor, not a list\n",
    "\n",
    "# select 3rd picture\n",
    "d0 = dataset.index_select(0,torch.tensor([3]))\n",
    "# different with d0\n",
    "d1 = dataset[1,:,:,:]\n",
    "# select 3rd, 4th, 5th columns of all pictures' all channels\n",
    "d2 = dataset.index_select(3,torch.tensor([3,4,5]))\n",
    "# the same with d1\n",
    "d3 = dataset[:,:,:,[3,4,5]]\n",
    "d_lst = [d0, d1, d2, d3]\n",
    "for i in d_lst:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7bf6e5",
   "metadata": {},
   "source": [
    "#### Attention: difference between slicing and indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd6b25fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 28, 28])\n",
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# slicing maintain the structure of original dataset\n",
    "Slicing = dataset[:1,...]\n",
    "print(Slicing.shape)\n",
    "# indexing can somehow decrease diemension\n",
    "Indexing = dataset[1,...]\n",
    "print(Indexing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4377000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 28, 28])\n",
      "torch.Size([4, 3, 28, 1])\n",
      "torch.Size([3, 28, 4])\n"
     ]
    }
   ],
   "source": [
    "'''...'''\n",
    "#... substitutes multiple :\n",
    "# select 2nd picture\n",
    "e0 = dataset[2:3,...]\n",
    "# select 0th - 3rd picures' all channels, all rows and 3rd columns\n",
    "e1 = dataset[0:4,...,3:4]\n",
    "# select 0th picture, all channels and all rows but 0th - 3rd columns\n",
    "e2 = dataset[0,...,:4]\n",
    "e_lst = [e0, e1, e2]\n",
    "for i in e_lst:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e2677c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False,  True,  True,  True],\n",
      "        [False,  True, False,  True],\n",
      "        [False,  True,  True,  True]])\n",
      "tensor([0.9486, 1.4917, 0.8320, 1.1588, 0.9822, 1.5929, 1.2281, 0.6945]) torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "'''select by mask'''\n",
    "# select all elements greater than 0.5\n",
    "ff = torch.randn(3,4)\n",
    "# method below can only return a boolean array\n",
    "mask = ff>0.5\n",
    "print(ff>0.5)\n",
    "# use the mask to select elements\n",
    "# return a 'flattened' tensor with desired elements\n",
    "f0 = torch.masked_select(ff, mask)\n",
    "print(f0, f0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302bd5a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
